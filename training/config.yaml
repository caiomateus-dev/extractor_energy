# Configurações de treinamento LoRA para MLX-VLM

# Modelo base (deve ser compatível com MLX-VLM)
base_model: "mlx-community/Qwen2.5-VL-7B-Instruct-4bit"

# Parâmetros LoRA
lora_layers: 16  # Número de camadas LoRA (ajuste conforme memória disponível)
lora_rank: 8     # Rank da matriz LoRA (maior = mais parâmetros, mais capacidade)
lora_alpha: 16   # Scaling factor para LoRA

# Parâmetros de treinamento
batch_size: 4
learning_rate: 1e-5
num_epochs: 3
warmup_steps: 100
gradient_accumulation_steps: 1

# Validação
val_split: 0.1  # 10% dos dados para validação

# Output
save_steps: 500  # Salvar checkpoint a cada N steps
save_total_limit: 3  # Manter apenas os últimos N checkpoints

# Logging
logging_steps: 50
report_to: "none"  # ou "wandb" se quiser usar Weights & Biases
